{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 5 Homework 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi0VFQqeYNgQ"
      },
      "source": [
        "!pip install scrapy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp2i1GHwTjp1"
      },
      "source": [
        "import requests\n",
        "from scrapy import Selector\n",
        "\n",
        "response = requests.get(\"https://cleantechnica.com/\")\n",
        "html_page = response.content\n",
        "html_sel = Selector(text=html_page)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0dz7T7ZZkLj"
      },
      "source": [
        "title = html_sel.css(\"h2.zox-s-title2::text\").extract()\n",
        "title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN5gcw-aW-uo"
      },
      "source": [
        "url = html_sel.css(\"div.zox-art-title > a::attr(href)\").extract()\n",
        "url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HHh5LM3boqI"
      },
      "source": [
        "import requests\n",
        "from scrapy import Selector\n",
        "\n",
        "def get_html(url):\n",
        "  response = requests.get(url)\n",
        "  content = response.content\n",
        "  return content\n",
        "\n",
        "def get_url(sel):\n",
        "  url = sel.css(\"div.zox-art-title > a::attr(href)\").extract_first()\n",
        "  return url\n",
        "\n",
        "def parse_content(sel):\n",
        "  data = {}\n",
        "  \n",
        "  # extract title\n",
        "  title = sel.css(\"div.zox-post-head > h1::text\").extract_first()\n",
        "  data[\"title\"] = title\n",
        "\n",
        "  # extract contents\n",
        "  # pakai 'or' karena beberapa berita memiliki perbedaan struktur element tags html-nya\n",
        "  contents = sel.css(\"div.zox-post-body > p > span::text\").extract() or sel.css(\"div.zox-post-body > p::text\").extract() \n",
        "  data[\"contents\"] = contents\n",
        "\n",
        "  # extract date\n",
        "  date = sel.css(\"span > time::attr(datetime)\").extract_first()\n",
        "  data[\"date\"] = date\n",
        "\n",
        "  # extract author\n",
        "  author = sel.css(\"div.zox-author-name-wrap > span > a::text\").extract_first()\n",
        "  data[\"author\"] = author\n",
        "\n",
        "  # extract url\n",
        "  url = sel.css(\"div.zox-article-wrap > meta::attr(itemid)\").extract_first()\n",
        "  data[\"url\"] = url\n",
        "\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yFGhdbjaJEh"
      },
      "source": [
        "html_page = get_html(\"https://cleantechnica.com/\")\n",
        "html_sel = Selector(text=html_page)\n",
        "\n",
        "news = html_sel.css(\"article.zox-art-wrap\")\n",
        "\n",
        "scraps = []\n",
        "for sel in news:\n",
        "  url = get_url(sel)\n",
        "  content_page = get_html(url) # langsung masuk ke masing-masing berita\n",
        "  content_sel = Selector(text=content_page)\n",
        "  data = parse_content(content_sel)\n",
        "\n",
        "  scraps.append(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgsSXCZsaUEQ"
      },
      "source": [
        "import pprint\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "\n",
        "for scrap in scraps:\n",
        "  pp.pprint(scrap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSuIFqXyDs1S"
      },
      "source": [
        "import json\n",
        "\n",
        "with open(\"cleantechnica_scrap.json\", \"w\") as file:\n",
        "  file.write(json.dumps(scraps))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}